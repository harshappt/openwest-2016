val list = sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10))
list.filter(_%2==0).collect()


val people = sc.textFile("us-500.csv")
val strArrRDD = people.map(line => line.split(","))
val stateTuples = strArrRDD.map(strs => (strs(6),1))

val countsByState = stateTuples.reduceByKey(_+_)
countsByState.take(30)

spark

val df = spark.read.format("csv").option("header","true").csv("us-500.csv")

df.printSchema

df.createOrReplaceTempView("people")


sql("select * from people").show

sql("select state,count(*) from people group by state").show(50)
sql("select state,count(*) from people group by state").sort.show(50)



sql("select state,count(*) count from people group by state order by count").show(50)


df.groupBy("state").count().orderBy("count").show(50)
import org.apache.spark.sql.types._

val longZip = df.withColumn("zip",df("zip").cast(IntegerType))

case class Person(firstName:String,lastName:String,companyName:String,address:String,city:String,county:String,state:String,zip:Integer)

val people = df.withColumn("zip",df("zip").cast(IntegerType)).as[Person]
people.filter(_.state == "CA").show
